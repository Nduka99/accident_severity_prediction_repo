--- Cell 3 (Start_Lat Drop) ---
class DataCleaning:
    def __init__(self):
        pass

    def execute(self, state):
        print(f'--- Executing {self.__class__.__name__} ---')
        # Restore state variables to local scope
        globals().update(state)

        # --- FIX: Explicitly retrieve state variables to prevent UnboundLocalError ---
        if 'df' in state: df = state['df']
        # -----------------------------------------------------------------------------

        #visualizing the datas missingness
        
        
        print("--- GENERATING JUSTIFICATION REPORT VISUALS ---")
        
        # --- VISUAL 1: THE "GHOST DATA" (Missing Values) ---
        # Justification for: Dropping 'Number', 'End_Lat', 'End_Lng', 'Wind_Chill'
        plt.figure(figsize=(12, 5))
        missing = df.isnull().mean() * 100
        missing = missing[missing > 5].sort_values(ascending=False) # Only show cols with >5% missing
        
        sns.barplot(x=missing.values, y=missing.index, palette='Reds_r')
        plt.title('Exhibit A: The "Ghost" Columns (>5% Missing)', fontsize=14, fontweight='bold')
        plt.xlabel('Percentage of Data Missing (%)')
        plt.axvline(50, color='black', linestyle='--')
        plt.text(52, 0, 'Critical Data Loss (>50%)', va='center')
        plt.tight_layout()
        plt.show()
        
       
        
        # --- VISUAL 2: THE "REDUNDANCY" CHECK (Twilight) ---
        # Justification for: Dropping Civil/Nautical/Astronomical Twilight
        # We create a cross-tabulation to show they are identical
        redundancy = pd.crosstab(df['Sunrise_Sunset'], df['Civil_Twilight'])
        
        plt.figure(figsize=(6, 4))
        sns.heatmap(redundancy, annot=True, fmt='d', cmap='Blues', cbar=False)
        plt.title('Exhibit B: The Redundancy Matrix\n(Sunrise_Sunset vs Civil_Twilight)', fontsize=12, fontweight='bold')
        plt.xlabel('Civil_Twilight')
        plt.ylabel('Sunrise_Sunset')
        plt.tight_layout()
        plt.show()
        
       
        
        # --- VISUAL 3: THE "SENSOR ROT" (Weather Gaps) ---
        # Justification for: Imputing by City+Time (Forward Fill) instead of Mean
        # Find a city with missing temperature to visualize the gap
        city_with_gaps = df[df['Temperature(F)'].isnull()]['City'].mode()[0]
        city_data = df[df['City'] == city_with_gaps].sort_values('Start_Time').iloc[:200] # Take first 200 records
        
        plt.figure(figsize=(12, 5))
        plt.plot(city_data['Start_Time'], city_data['Temperature(F)'], marker='o', linestyle='-', color='gray', alpha=0.5, label='Recorded Temp')
        # Highlight missing values
        missing_dates = city_data[city_data['Temperature(F)'].isnull()]['Start_Time']
        plt.vlines(missing_dates, ymin=city_data['Temperature(F)'].min(), ymax=city_data['Temperature(F)'].max(), 
                   colors='red', alpha=0.5, label='MISSING DATA (Sensor Failure)')
        
        plt.title(f'Sensor Failure Pattern ({city_with_gaps})', fontsize=14, fontweight='bold')
        plt.ylabel('Temperature (F)')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
        
      
        
        # --- VISUAL 4: THE "KILL ZONE" (Duration vs Severity) ---
        # Justification for: Keeping Extreme Duration Rows (>4 Hours)
        df['End_Time'] = pd.to_datetime(df['End_Time'])
        df['Duration_Minutes'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60
        df['Duration_Group'] = np.where(df['Duration_Minutes'] > 240, 'Extreme (>4h)', 'Normal (<4h)')
        
        # Calculate Fatality Rate
        fatality_rate = df.groupby('Duration_Group')['Severity'].apply(lambda x: (x==4).mean() * 100).reset_index()
        
        plt.figure(figsize=(8, 5))
        sns.barplot(x='Duration_Group', y='Severity', data=fatality_rate, palette=['#2ECC71', '#E74C3C'])
        plt.title('Exhibit D: The "Kill Zone"\n(% Fatalities by Duration)', fontsize=14, fontweight='bold')
        plt.ylabel('Percentage of Fatal Accidents (%)')
        plt.tight_layout()
        plt.show()
        
        

        
        print(f"--- DATASET AUDIT REPORT ---")
        print(f"Total Rows: {len(df):,}")
        print(f"Total Columns: {len(df.columns)}")
        
        # 2. THE TIME TRUTH (Verify the Date Range)
        # We must convert Start_Time to datetime to check the months
        df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')
        df['Month'] = df['Start_Time'].dt.month_name()
        
        print("\n--- 1. TEMPORAL COVERAGE (The Seasonality Trap) ---")
        print(f"Earliest Date: {df['Start_Time'].min()}")
        print(f"Latest Date:   {df['Start_Time'].max()}")
        print("\nRecords by Month (Is it just Winter?):")
        print(df['Month'].value_counts())
        
        # 3. THE TARGET REALITY (Severity Imbalance)
        print("\n--- 2. TARGET VARIABLE (Severity) ---")
        severity_counts = df['Severity'].value_counts(normalize=True).sort_index()
        print(severity_counts)
        
        # Visualize Severity
        plt.figure(figsize=(8, 4))
        sns.countplot(x='Severity', data=df, palette= severity_palette)
        plt.title('The Class Imbalance Problem: Severity Distribution', fontweight='bold')
        plt.show()
        
        # 4. THE ROT (Missing Values Analysis)
        # We only care about columns with > 0% missing
        missing = df.isnull().mean() * 100
        missing = missing[missing > 0].sort_values(ascending=False)
        
        print("\n--- 3. MISSING VALUES (% of Data) ---")
        if len(missing) > 0:
            print(missing.head(15))
            
            # Visualizing the Gap
            plt.figure(figsize=(10, 6))
            missing.head(15).plot(kind='barh', color='#E74C3C')
            plt.title('Top 15 Columns with Missing Data (%)', fontweight='bold')
            plt.xlabel('Percentage Missing')
            plt.gca().invert_yaxis() # Highest missing at top
            plt.show()
        else:
            print("Incredible. No missing values found (Highly unlikely).")
        
        # 5. CARDINALITY CHECK (Categorical Complexity)
        # Which columns have too many unique values to encode?
        print("\n--- 4. CATEGORICAL COMPLEXITY (Top 10 High Cardinality) ---")
        cat_cols = df.select_dtypes(include=['object']).columns
        cardinality = df[cat_cols].nunique().sort_values(ascending=False)
        print(cardinality.head(10))
        
        # 6. NUMERICAL SUMMARY
        print("\n--- 5. NUMERICAL STATS (Outliers Check) ---")
        # Transpose for readability
        print(df.describe().T[['mean', 'min', '50%', 'max', 'std']])

        df.isnull().sum()

        
        #fix city missing values
        
        # 1. Split data into 'Known' and 'Unknown' Cities
        # We only use rows that have valid coordinates for training
        known_cities = df[df['City'].notna() & df['Start_Lat'].notna()].copy()
        unknown_cities = df[df['City'].isna() & df['Start_Lat'].notna()].copy()
        
        print(f"Fixing {len(unknown_cities)} rows with missing cities...")
        
        if not unknown_cities.empty:
            # 2. Prepare the Model
            # We use Latitude and Longitude to predict the City
            X_train = known_cities[['Start_Lat', 'Start_Lng']]
            y_train = known_cities['City']
        
            # 3. Train KNN Classifier
            # n_neighbors=1 means "Just take the name of the single closest accident"
            knn = KNeighborsClassifier(n_neighbors=1)
            knn.fit(X_train, y_train)
        
            # 4. Predict the missing cities
            X_missing = unknown_cities[['Start_Lat', 'Start_Lng']]
            predicted_cities = knn.predict(X_missing)
        
            # 5. Fill the gaps in the main dataframe
            # We iterate nicely to ensure indices match
            df.loc[unknown_cities.index, 'City'] = predicted_cities
        
            print(f"Success! {len(unknown_cities)} missing cities have been imputed based on location.")
            
            # Optional: Verify one or two
            print("Example fix:", df.loc[unknown_cities.index[0], ['Start_Lat', 'Start_Lng', 'City']])
        
        else:
            print("No missing cities found (or they lack coordinates).")

        df['City'].to_list()

        df.duplicated().sum()

        
        
        # 1. Filter for one city (Visualizing all cities at once is too messy)
        city_name = 'Pasadena'  # Change this to a city with missing data you want to inspect
        city_df = df[df['City'] == city_name].copy()
        
        # Ensure Start_Time is datetime
        city_df['Start_Time'] = pd.to_datetime(city_df['Start_Time'])
        city_df = city_df.sort_values('Start_Time')
        
        # Variables to visualize
        features = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Wind_Chill(F)']
        
        # 2. Setup the Plot (One subplot per feature)
        fig, axes = plt.subplots(nrows=len(features), ncols=1, figsize=(15, 12), sharex=True)
        plt.subplots_adjust(hspace=0.3)
        fig.suptitle(f'Weather Sensor Failure Patterns: {city_name}', fontsize=16, weight='bold')
        
        for i, col in enumerate(features):
            ax = axes[i]
            
            # Plot valid data as grey dots
            valid_data = city_df.dropna(subset=[col])
            ax.scatter(valid_data['Start_Time'], valid_data[col], alpha=0.6, color='grey', s=10, label='Recorded')
            
            # Identify missing data rows
            missing_data = city_df[city_df[col].isna()]
            
            # Plot vertical red lines for missing data timestamps
            if not missing_data.empty:
                # We use vlines to create the "Barcode" effect seen in Exhibit C
                ax.vlines(x=missing_data['Start_Time'], 
                          ymin=city_df[col].min(), 
                          ymax=city_df[col].max(), 
                          colors='red', alpha=0.3, linewidth=1, label='MISSING')
        
            ax.set_ylabel(col, fontsize=10)
            ax.grid(True, linestyle='--', alpha=0.5)
            
            # Only add legend to the first plot to avoid clutter
            if i == 0:
                ax.legend(loc='upper right')
        
        plt.xlabel('Date/Time', fontsize=12)
        plt.show()

        
        
        
        # ==========================================
        # PART 1: Automatically Create the Neighbor Map
        # ==========================================
        
        # 1. Get the "Centroid" (Average Lat/Lng) for each city
        # This gives us a single coordinate point representing each city
        city_coords = df.groupby('City')[['Start_Lat', 'Start_Lng']].mean().reset_index()
        
        # 2. Prepare data for the BallTree (Needs Radians for Haversine distance)
        # We convert Lat/Lng to radians
        coords_rad = np.deg2rad(city_coords[['Start_Lat', 'Start_Lng']].values)
        
        # 3. Build the Tree
        # metric='haversine' calculates distance on a sphere (Earth)
        tree = BallTree(coords_rad, metric='haversine')
        
        # 4. Query the Tree
        # k=2 because the closest neighbor to a city is ITSELF (distance 0). 
        # We want the 2nd closest.
        distances, indices = tree.query(coords_rad, k=2)
        
        # 5. Build the Dictionary
        # Earth Radius approx 3963 miles
        earth_radius_miles = 3963
        neighbor_map = {}
        
        for i, (dist_rad, idx) in enumerate(zip(distances, indices)):
            city_a = city_coords.iloc[i]['City']
            city_b_idx = idx[1] # The index of the neighbor
            city_b = city_coords.iloc[city_b_idx]['City']
            
            distance_miles = dist_rad[1] * earth_radius_miles
            
            # SAFETY CHECK: Only map if the neighbor is within 50 miles
            # You don't want to map New York to a city 500 miles away just because it's the "nearest"
            if distance_miles <= 50:
                neighbor_map[city_a] = city_b
        
        print(f"Automatically mapped {len(neighbor_map)} cities to their nearest neighbors.")
        print("Sample mappings:", dict(list(neighbor_map.items())[:5]))
        
        
        # ==========================================
        # PART 2: Apply the Map (Same Logic as Before)
        # ==========================================
        
        # Sort for merge_asof
        df = df.sort_values('Start_Time')
        valid_weather_data = df.dropna(subset=['Temperature(F)']).copy()
        
        # List of columns you want to fill
        weather_cols = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Wind_Chill(F)']
        
        # Counter for tracking progress
        total_filled = 0
        
        for target_city, source_city in neighbor_map.items():
            
            # Only proceed if target city actually has missing data
            target_mask = (df['City'] == target_city) & (df['Temperature(F)'].isna())
            
            # Skip if no missing data (optimization)
            if not target_mask.any():
                continue
                
            missing_rows = df[target_mask].copy()
            source_rows = valid_weather_data[valid_weather_data['City'] == source_city]
            
            if source_rows.empty:
                continue
        
            # Perform the time-based merge
            filled_rows = pd.merge_asof(
                missing_rows,
                source_rows[['Start_Time'] + weather_cols],
                on='Start_Time',
                direction='nearest',
                tolerance=pd.Timedelta('1 hour'), # Strict time limit
                suffixes=('', '_neighbor')
            )
            
            # Update the dataframe
            # We use the index to push values back
            for idx, row in filled_rows.iterrows():
                original_idx = missing_rows.iloc[idx].name
                
                # If we found a match (neighbor data isn't NaN)
                if pd.notna(row[f'Temperature(F)_neighbor']):
                    for col in weather_cols:
                        # Assign the neighbor's value to the original dataframe
                        df.at[original_idx, col] = row[f'{col}_neighbor']
                    total_filled += 1
        
        print(f"\nTotal rows filled using Spatial Imputation: {total_filled}")

        df[weather_cols].isnull().sum()

        #handle remaining missing weather numerical values using interpolation
        
        # 1. Prepare Data
        # Ensure we are sorted by City and Time, otherwise interpolation fails
        df = df.sort_values(by=['City', 'Start_Time'])
        
        # Columns to fix (excluding Wind_Chill for now)
        cols_to_fix = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']
        
        print(f"Missing before 1-Year Cascade:\n{df[cols_to_fix].isna().sum()}")
        
        # ==================================================
        # STEP 1: Time-Based Interpolation (The "Connect the Dots" Fill)
        # ==================================================
        # If the sensor works at 1pm (70F) and 3pm (72F), we assume 2pm was 71F.
        # limit_direction='both' fixes the start/end of the dataset too.
        # limit=3 ensures we don't draw lines across massive month-long gaps (which would be inaccurate).
        
        print("Step 1: Interpolating short gaps...")
        # We group by City to ensure we don't interpolate between two different cities
        df[cols_to_fix] = df.groupby('City')[cols_to_fix].transform(
            lambda group: group.interpolate(method='linear', limit=3, limit_direction='both')
        )
        
        # ==================================================
        # STEP 2: City-Specific Month-Hour Average
        # ==================================================
        # "What is the average 2pm temperature for this city in this month?"
        # (Uses the ~30 days of data available in your 1 year)
        
        df['Month'] = df['Start_Time'].dt.month
        df['Hour'] = df['Start_Time'].dt.hour
        
        print("Step 2: Filling with Month-Hour Average...")
        df[cols_to_fix] = df.groupby(['City', 'Month', 'Hour'])[cols_to_fix].transform(
            lambda x: x.fillna(x.mean())
        )
        
        # ==================================================
        # STEP 3: City-Specific Monthly Average (Backup)
        # ==================================================
        # If the sensor was dead at 2pm for the WHOLE month, Step 2 fails (mean is NaN).
        # We fallback to the average of the entire month (ignoring the hour).
        
        print("Step 3: Filling with Monthly Average...")
        df[cols_to_fix] = df.groupby(['City', 'Month'])[cols_to_fix].transform(
            lambda x: x.fillna(x.mean())
        )
        
        # ==================================================
        # STEP 4: Global Month-Hour Average (Last Resort)
        # ==================================================
        # If the city was dead for the WHOLE MONTH, we look at the global average
        # for that month/hour across all other cities.
        
        print("Step 4: Filling with Global Month-Hour Average...")
        for col in cols_to_fix:
            if df[col].isna().any():
                global_means = df.groupby(['Month', 'Hour'])[col].transform('mean')
                df[col] = df[col].fillna(global_means)
        
        # ==================================================
        # STEP 5: Wind Chill Logic
        # ==================================================
        # As discussed, Wind Chill is not a sensor. If it's missing, it implies
        # the weather wasn't cold/windy enough to trigger it. 
        # We default it to the Temperature.
        
        print("Step 5: Fixing Wind Chill...")
        df['Wind_Chill(F)'] = df['Wind_Chill(F)'].fillna(df['Temperature(F)'])
        
        # Final Check
        print(f"\nRemaining Missing Values:\n{df[cols_to_fix + ['Wind_Chill(F)']].isna().sum()}")

        df.isnull().sum()

        df['Precipitation(in)']

        df['Precipitation(in)'].isnull().sum()

        
        
        # ==========================================
        # PART 1: Spatial Fill (Catch the missing storms)
        # ==========================================
        # We re-use the neighbor_map logic, but ONLY for Precipitation this time.
        # If you already have the 'neighbor_map' from the previous step, this works automatically.
        
        # If you need to re-define the map manually for testing:
        # neighbor_map = {'Pasadena': 'Los Angeles', 'Burbank': 'Glendale', ...} 
        
        print("Step 1: Checking neighbors for missing storms...")
        
        # Sort for merge_asof
        df = df.sort_values('Start_Time')
        valid_rain_data = df.dropna(subset=['Precipitation(in)']).copy()
        
        # We only care about borrowing rain data if the neighbor actually reported rain (>0)
        # Borrowing "0" is redundant because we will fill with 0 anyway.
        rainy_neighbors = valid_rain_data[valid_rain_data['Precipitation(in)'] > 0]
        
        for target_city, source_city in neighbor_map.items():
            
            # Identify gaps in target city
            target_mask = (df['City'] == target_city) & (df['Precipitation(in)'].isna())
            missing_rows = df[target_mask]
            
            if missing_rows.empty:
                continue
        
            # Get data from the specific source city (only if it was raining there)
            source_rows = rainy_neighbors[rainy_neighbors['City'] == source_city]
            
            if source_rows.empty:
                continue
        
            # Map the rain
            filled_rows = pd.merge_asof(
                missing_rows,
                source_rows[['Start_Time', 'Precipitation(in)']],
                on='Start_Time',
                direction='nearest',
                tolerance=pd.Timedelta('30 minutes'), # Rain is local; keep tolerance tight
                suffixes=('', '_neighbor')
            )
        
            # Update the main dataframe
            for idx, row in filled_rows.iterrows():
                original_idx = missing_rows.iloc[idx].name
                if pd.notna(row['Precipitation(in)_neighbor']):
                    df.at[original_idx, 'Precipitation(in)'] = row['Precipitation(in)_neighbor']
        
        print("   - Spatial rain imputation complete.")
        
        # ==========================================
        # PART 2: The "Silent Zero" Fill
        # ==========================================
        # Any remaining NaNs imply no rain was recorded or no nearby storm was found.
        # Safest assumption is 0.00.
        
        missing_count = df['Precipitation(in)'].isna().sum()
        print(f"Step 2: Filling {missing_count} remaining gaps with 0.00 (No Rain)...")
        
        df['Precipitation(in)'] = df['Precipitation(in)'].fillna(0.0)
        
        print(f"Final check: Missing Precipitation = {df['Precipitation(in)'].isna().sum()}")

        print(df.columns.to_list())

        # --- STEP 3.0: DOMAIN-BACKED CLEANING (FINAL EXECUTION) ---
        
        
        df2 = df.copy()
        print(f"Original Count: {len(df):,}")
        
        
        
        # 3. COLUMN PURGE
        # Drop 'Ghost' lat/lng and redundant Twilights, and other columns that cause leakage
        cols_to_drop = ['Number', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 
                        'End_Lat', 'End_Lng', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp',
                        'Duration', 'Distance(mi)']
        df = df.drop(columns=cols_to_drop, errors='ignore')
        
        
        # 5. BINARY MAPPING and Sunrise_Sunset Missing values handling
        # 1. Check how many we are fixing
        missing_mask = df2['Sunrise_Sunset'].isna()
        print(f"Fixing {missing_mask.sum()} missing Sunrise_Sunset values using 'Hour'...")
        
        
        accident_hour = df2['Hour']
        
        # 3. The Logic: Standard definitions
        # Day = 6 AM to 6 PM (Hours 6-17)
        # Night = 6 PM to 6 AM (Hours 18-23, 0-5)
        
        # Create a filler series based on hour
        # Returns 'Night' if hour >= 18 OR hour < 6, else 'Day'
        imputed_values = accident_hour.apply(lambda h: 'Night' if (h >= 18 or h < 6) else 'Day')
        
        # 4. Fill ONLY the missing values
        df2['Sunrise_Sunset'] = df2['Sunrise_Sunset'].fillna(imputed_values)
        
        # 5. Now safely map to Binary
        df2['Is_Night'] = df2['Sunrise_Sunset'].map({'Night': 1, 'Day': 0})
        # Map ensures any remaining weird text becomes NaN (safeguard), but fillna fixed it.
        
        # Drop original column
        df2_cleaned = df2.drop(columns=['Sunrise_Sunset'])
        
        print(f"Is_Night Distribution:\n{df2['Is_Night'].value_counts(normalize=True)}")
        
        
        # 6. SAVE
        df2_cleaned.to_csv('us_accidents_2023_cleaned.csv', index=False)
        print("--- CLEANING COMPLETE: Saved to 'us_accidents_2023_cleaned.csv' ---")

        cols_to_drop2 = ['ID', 'Zipcode']
        df2_cleaned = df2_cleaned.drop(columns=cols_to_drop, errors='ignore')

        df2_cleaned.isnull().sum()


        cols = ['Wind_Direction', 'Weather_Condition', 'Source']
        df2_cleaned[cols].nunique()

        df2_cleaned['Source'].unique()

        df2_cleaned['Wind_Direction'].unique()

        df2_cleaned['Weather_Condition'].unique()

        #handling weather)condition 26984 missing values
        # 1. Identify the missing rows
        missing_mask = df2_cleaned['Weather_Condition'].isna()
        print(f"Handling {missing_mask.sum()} missing weather descriptions using sensor data...")
        
        # 2. PROXY 1: If it's freezing and precipitating, it's SNOW
        # (We assume < 32F is freezing)
        snow_mask = missing_mask & (df2_cleaned['Precipitation(in)'] > 0) & (df2_cleaned['Temperature(F)'] <= 32)
        df2_cleaned.loc[snow_mask, 'Weather_Condition'] = 'Light Snow' # Specific text for the simplifier to catch
        
        # 3. PROXY 2: If it's warm and precipitating, it's RAIN
        rain_mask = missing_mask & (df2_cleaned['Precipitation(in)'] > 0) & (df2_cleaned['Temperature(F)'] > 32)
        df2_cleaned.loc[rain_mask, 'Weather_Condition'] = 'Light Rain'
        
        # 4. PROXY 3: If no precipitation, assume CLEAR
        # This catches the remaining NaNs
        df2_cleaned['Weather_Condition'] = df2_cleaned['Weather_Condition'].fillna('Clear')
        
        print("   -> Imputation Complete.")
        print(f"   -> Reclaimed {snow_mask.sum()} hidden Snow events.")
        print(f"   -> Reclaimed {rain_mask.sum()} hidden Rain events.")
        
        # --- NOW RUN YOUR SIMPLIFICATION FUNCTION ---
        # (Paste your simplify_weather function and apply code here)
        # ...

        '''
        New Category,   Underlying Physics,   Original Terms (Examples)
        Snow/Ice,Low      Friction (Deadly),  "Snow, Sleet, Ice, Freezing Rain, Wintry Mix, Hail"
        Storm,          Chaos (Wind + Wet),    "Thunder, T-Storm, Tornado, Squalls"
        Rain,          Medium Friction (Wet),   "Rain, Drizzle, Showers"
        Fog/Obscured,     Low Visibility,       "Fog, Mist, Haze, Smoke, Dust"
        Cloudy,             Benign,             "Cloudy, Overcast"
        Clear,             Baseline,              "Fair, Clear, Partly Cloudy, N/A"
        '''


        # Create a copy to ensure we don't mess up the original pointer
        df_cleaned2 = df2_cleaned.copy()
        
        # --- 1. WIND DIRECTION FIX ---
        # Domain Logic: If wind data is missing, it is standard to assume "CALM" (0 speed).
        # We keep the 16 compass points (N, NE, SW...) as they interact with road geometry.
        df_cleaned2['Wind_Direction'] = df_cleaned2['Wind_Direction'].fillna('CALM')
        
        # --- 2. WEATHER CONDITION SIMPLIFICATION ---
        # Domain Logic: Reduce 91+ text descriptions to 6 categories based on "Road Friction" and "Visibility".
        
        def simplify_weather(w):
            # Handle NaN: Default to 'Clear' (Baseline condition)
            if pd.isna(w):
                return 'Clear'
            
            w = str(w).lower()
            
            # PRIORITY 1: FROZEN PRECIPITATION (Highest Danger / Lowest Friction)
            # Includes: Light Snow, Heavy Ice Pellets, Wintry Mix, Freezing Rain
            if any(x in w for x in ['snow', 'sleet', 'ice', 'freezing', 'wintry', 'hail']):
                return 'Snow/Ice'
            
            # PRIORITY 2: SEVERE STORM (High Wind + Rain + Unpredictability)
            # Includes: T-Storm, Thunder, Tornado, Funnel Cloud, Squalls
            if any(x in w for x in ['thunder', 't-storm', 'tornado', 'squall']):
                return 'Storm'
            
            # PRIORITY 3: RAIN (Wet Roads / Medium Friction)
            # Includes: Light Rain, Drizzle, Showers, Heavy Rain
            if any(x in w for x in ['rain', 'drizzle', 'shower']):
                return 'Rain'
            
            # PRIORITY 4: OBSCURITY (Visual Hazard / Dry or Damp Surface)
            # Includes: Fog, Mist, Haze, Smoke, Dust, Sand
            if any(x in w for x in ['fog', 'mist', 'haze', 'smoke', 'dust', 'sand']):
                return 'Fog/Obscured'
            
            # PRIORITY 5: CLOUDY (Benign, No Precipitation)
            # Includes: Overcast, Mostly Cloudy, Scattered Clouds
            if any(x in w for x in ['cloudy', 'overcast']):
                return 'Cloudy'
            
            # PRIORITY 6: CLEAR (Default / Safe)
            # Includes: Fair, Clear, N/A Precipitation
            return 'Clear'
        
        # Apply the logic
        df_cleaned2['Weather_Simplified'] = df_cleaned2['Weather_Condition'].apply(simplify_weather)
        
        # Drop the old 91-value column to save memory
        df_cleaned2 = df_cleaned2.drop(columns=['Weather_Condition'])
        
        # --- 3. VERIFICATION ---
        print("--- CLEANING COMPLETE ---")
        print(f"New Weather Categories:\n{df_cleaned2['Weather_Simplified'].value_counts()}")
        print(f"\nRemaining Missing Wind Values: {df_cleaned2['Wind_Direction'].isnull().sum()}")

        df_cleaned2.isnull().any(axis=1).sum()

        df_cleaned2.isnull().sum()

        # Create a new version for this step
        df_cleaned3 = df_cleaned2.copy()
        
        print("--- HANDLING SPATIAL TEXT FEATURES ---")
        
        
        print(f"Missing 'Street' before repair: {df_cleaned3['Street'].isna().sum()}")
        
        # ==========================================
        # PHASE 1: SPATIAL REPAIR (Impute Missing Streets)
        # ==========================================
        # Logic: If accident A is at (Lat, Lng) and missing 'Street', 
        # but accident B is at the same (Lat, Lng) and has 'Street', copy it.
        
        # 1. Identify rows with known vs. unknown streets
        known_streets = df_cleaned3[df_cleaned3['Street'].notna() & df_cleaned3['Start_Lat'].notna()].copy()
        unknown_streets = df_cleaned3[df_cleaned3['Street'].isna() & df_cleaned3['Start_Lat'].notna()].copy()
        
        if not unknown_streets.empty and not known_streets.empty:
            print("Attempting to impute missing street names using coordinates...")
            
            # 2. Train a simple Nearest Neighbor model
            # We use k=1 to find the single closest accident
            X_train = known_streets[['Start_Lat', 'Start_Lng']]
            y_train = known_streets['Street']
            
            knn = KNeighborsClassifier(n_neighbors=1, weights='distance')
            knn.fit(X_train, y_train)
            
            # 3. Predict the missing streets
            predicted_streets = knn.predict(unknown_streets[['Start_Lat', 'Start_Lng']])
            
            # 4. Fill the gaps
            df_cleaned3.loc[unknown_streets.index, 'Street'] = predicted_streets
            print(f"Repaired {len(unknown_streets)} missing Street values.")
        
        # ==========================================
        # PHASE 2: HIGHWAY DETECTION (Feature Extraction)
        # ==========================================
        # Standard list of US Highway indicators
        highway_keywords = [
            'I-', 'Interstate', 'Hwy', 'Highway', 'Route', 'St', 'State', 
            'Turnpike', 'Fwy', 'Freeway', 'Expy', 'Expressway', 'Pkwy', 'Parkway',
            'US-', 'U.S.', 'Loop', 'Bypass'
        ]
        
        def detect_highway(street_name):
            # Handle stubborn NaNs (if repair failed)
            if pd.isna(street_name):
                return 0 # Safe assumption: Missing usually implies smaller/local roads
            
            street_name = str(street_name).lower()
            
            # Check for keywords
            if any(keyword.lower() in street_name for keyword in highway_keywords):
                return 1
            return 0
        
        # Apply the detector to the now-repaired column
        df_cleaned3['Is_Highway'] = df_cleaned3['Street'].apply(detect_highway)
        
        # Drop the high-cardinality text column
        if 'Street' in df_cleaned3.columns:
            df_cleaned3 = df_cleaned3.drop(columns=['Street'])
        
        # --- FINAL AUDIT ---
        print("\n--- STREET TRANSFORMATION COMPLETE ---")
        print(f"Is_Highway Dist:\n{df_cleaned3['Is_Highway'].value_counts(normalize=True)}")

        df_cleaned3

        
        df_cleaned3 = df_cleaned3.drop(columns=['Source'], errors='ignore')
        df_cleaned3

        df_cleaned3.isnull().any(axis=1).sum()

        df_cleaned3 = df_cleaned3.drop(columns=['ID'], errors='ignore')
        df_cleaned3 = df_cleaned3.drop(columns=['Zipcode'], errors='ignore')
        df_cleaned3.head()

        print(df_cleaned3.columns.to_list())

        df_cleaned3['Description'].sample(20)

        
        # Use the dataframe from the current step
        df_viz = df_cleaned3.copy()
        
        poi_cols = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 
                    'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 
                    'Traffic_Signal', 'Turning_Loop']
        
        print("--- GENERATING POI VARIANCE VISUALIZATION ---")
        
        # 1. Calculate True Ratios
        data = []
        for col in poi_cols:
            if col in df_viz.columns:
                true_count = df_viz[col].sum()
                ratio = (true_count / len(df_viz)) * 100
                
                # Assign Color Category based on Academic Thresholds
                if ratio < 0.001:
                    category = 'Dead (<0.001%)'
                    color = '#E74C3C' # Red
                elif ratio < 1.0:
                    category = 'Rare (0.001% - 1%)'
                    color = '#F1C40F' # Yellow
                else:
                    category = 'Robust (>1%)'
                    color = '#2ECC71' # Green
                    
                data.append({'Feature': col, 'True_Ratio': ratio, 'Color': color, 'Category': category})
        
        df_plot = pd.DataFrame(data).sort_values('True_Ratio', ascending=True)
        
        # 2. Plot
        plt.figure(figsize=(12, 8))
        bars = plt.barh(df_plot['Feature'], df_plot['True_Ratio'], color=df_plot['Color'])
        
        # 3. Aesthetics
        plt.xscale('log') # CRITICAL: Use Log Scale to see the tiny values
        plt.xlabel('Percentage of "True" Cases (Log Scale)', fontsize=12, fontweight='bold')
        plt.title('The "Variance Spectrum": Identifying Dead vs. Robust Features', fontsize=16, fontweight='bold')
        
        # Add Threshold Lines
        plt.axvline(x=0.001, color='red', linestyle='--', linewidth=1, alpha=0.7)
        plt.text(0.0012, 0, 'Dead Zone (0.001%)', color='red', rotation=90, va='bottom')
        
        plt.axvline(x=1.0, color='green', linestyle='--', linewidth=1, alpha=0.7)
        plt.text(1.2, 0, 'Robust Zone (1%)', color='green', rotation=90, va='bottom')
        
        # Legend
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='#E74C3C', label='Dead / Noise (<0.001%)'),
                           Patch(facecolor='#F1C40F', label='Rare Signal (0.001% - 1%)'),
                           Patch(facecolor='#2ECC71', label='Robust Feature (>1%)')]
        plt.legend(handles=legend_elements, loc='lower right')
        
        plt.grid(axis='x', which='both', linestyle='--', alpha=0.3)
        plt.tight_layout()
        plt.show()

        
        
        # 1. Select the features to check
        # We include Boolean POIs, Weather, and new features like Is_Highway
        # (Ensure categorical text columns like 'Weather_Simplified' are One-Hot Encoded 
        # or excluded for this specific numeric check. Here we focus on Boolean/Numeric)
        
        features_to_check = [
            'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 
            'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 
            'Traffic_Signal', 'Is_Highway'
        ]
        
        # Filter df for these columns
        # (Assuming df_cleaned3 is your current dataframe)
        corr_df = df_cleaned3[features_to_check]
        
        # 2. Calculate Correlation Matrix
        # method='spearman' is often better for boolean/skewed data than 'pearson', 
        # but 'pearson' is standard. Let's use Pearson for simplicity.
        corr_matrix = corr_df.corr()
        
        # 3. Plot the Heatmap
        plt.figure(figsize=(16, 12))
        sns.heatmap(corr_matrix, 
                    annot=True,       # Show numbers
                    fmt=".2f",        # 2 decimal places
                    cmap='coolwarm',  # Red = High Positive, Blue = High Negative
                    vmin=-1, vmax=1,  # Set scale
                    linewidths=0.5)
        
        plt.title('Feature Correlation Matrix: Detecting Redundancy', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # 4. Automatic "Red Flag" Detector
        # Print pairs with correlation > 0.7 (Standard threshold for "Too Similar")
        print("--- HIGH CORRELATION PAIRS (>0.7) ---")
        threshold = 0.7
        
        # Iterate through the matrix to find high correlations (excluding self-correlation)
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i):
                if abs(corr_matrix.iloc[i, j]) > threshold:
                    colname_i = corr_matrix.columns[i]
                    colname_j = corr_matrix.columns[j]
                    score = corr_matrix.iloc[i, j]
                    high_corr_pairs.append((colname_i, colname_j, score))
        
        if not high_corr_pairs:
            print("No highly correlated pairs found. All features are sufficiently distinct.")
        else:
            for pair in high_corr_pairs:
                print(f"RED FLAG: {pair[0]} vs {pair[1]} | Score: {pair[2]:.2f}")

        # Create df_poi_audit for this specific task
        df_poi_audit = df_cleaned3.copy()
        
        poi_cols = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 
                    'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 
                    'Traffic_Signal', 'Turning_Loop']
        
        print("--- ACADEMIC POI VARIANCE AUDIT ---")
        print(f"Total Rows: {len(df_poi_audit):,}")
        print("-" * 60)
        print(f"{'COLUMN':<20} | {'TRUE COUNT':<10} | {'RATIO (%)':<10} | {'VERDICT'}")
        print("-" * 60)
        
        cols_to_drop_final = []
        
        for col in poi_cols:
            if col in df_poi_audit.columns:
                true_count = df_poi_audit[col].sum()
                ratio = (true_count / len(df_poi_audit)) * 100
                
                # LOGIC TIER 1: DEAD (Near-Zero Variance)
                # Threshold: Less than 0.001% (approx 10-15 rows in 1.2M)
                if ratio < 0.001: 
                    verdict = "DROP (Dead/Noise)"
                    cols_to_drop_final.append(col)
                    
                # LOGIC TIER 2: RARE (Between 0.001% and 1%)
                # Threshold: We flag it, but usually KEEP because XGBoost can handle sparsity
                elif ratio < 1.0:
                    verdict = "KEEP (Rare Signal)"
                    
                # LOGIC TIER 3: ROBUST (> 1%)
                else:
                    verdict = "KEEP (Robust)"
                    
                print(f"{col:<20} | {true_count:<10,} | {ratio:<10.5f} | {verdict}")
            
        
        print("-" * 60)
        
        # Execute the Drops
        if cols_to_drop_final:
            print(f"\nDropping {len(cols_to_drop_final)} columns based on Near-Zero Variance...")
            df_poi_audit = df_poi_audit.drop(columns=cols_to_drop_final)
            print(f"\nDropping Bump due to multicollinearity")
            df_poi_audit = df_poi_audit.drop(columns=['Bump'])
            print(f"Dropped: {cols_to_drop_final}")
        else:
            print("\nNo columns met the 'Dead' criteria.")
            
        # Final Check
        print(f"\nRemaining Columns: {len(df_poi_audit.columns)}")

        print(df_poi_audit.columns.to_list())

        df_final = df_poi_audit.copy()
        df_final.head()


        # Update state with current locals (naively saving everything)
        # Filtering to avoid saving self and state itself
        state.update({k: v for k, v in locals().items() if k not in ['self', 'state']})
        return state

--------------------
--- Cell 6 (Text Features) ---
class FeatureEngineering:
    def __init__(self):
        pass

    def execute(self, state):
        print(f'--- Executing {self.__class__.__name__} ---')
        # Restore state variables to local scope
        globals().update(state)

        # --- FIX: Explicitly retrieve state variables to prevent UnboundLocalError ---
        if 'df_final' in state: df_final = state['df_final']
        # -----------------------------------------------------------------------------

        
        
        
        # Use a copy to avoid SettingWithCopy warnings
        df_text = df_final.copy()
        
        # Ensure text is lowercase for matching
        df_text['desc_clean'] = df_text['Description'].str.lower().fillna('')
        
        print("--- STARTING REAL-TIME TEXT ANALYSIS ---")
        
        # ==============================================================================
        # STRATEGY 1: THE WHITELIST (Safe Features known at T=0)
        # ==============================================================================
        # We only create columns for these specific, real-time physical states.
        
        safe_keywords = {
            # 1. TRAFFIC STATUS (The "Effect")
            'Desc_Queue':     r'\b(queue|backups?|slow|stationary|stop|waiting|delays?)\b',
            'Desc_Heavy':     r'\b(heavy|congestion|gridlock|bumper)\b',
            
            # 2. PHYSICAL BLOCKAGE (The "Cause")
            'Desc_Blocked':   r'\b(block|close|lane|closed|shut|down)\b',
            'Desc_Ramp':      r'\b(ramp|exit|entry|interchange)\b',
            
            # 3. INCIDENT TYPE (The "Event")
            'Desc_Accident':  r'\b(accident|crash|collision|incident)\b',
            'Desc_Hazard':    r'\b(hazard|debris|object|spill|obstacle|animal)\b',
            
            # 4. URGENCY / SPECIFICS
            'Desc_Caution':   r'\b(caution|care|alert|warning)\b',
            'Desc_Fire':      r'\b(fire|smoke|flame|burn)\b'
        }
        
        print("Extracting Real-Time Whitelist Features...")
        for col_name, pattern in safe_keywords.items():
            # Create 1/0 Feature
            df_text[col_name] = df_text['desc_clean'].str.contains(pattern, regex=True).astype(int)
            
            # Check "Variance": If a column is 99.9% 0s, it might be useless (optional check)
            count = df_text[col_name].sum()
            print(f"   -> Feature '{col_name}' detected in {count} rows.")
        
        # ==============================================================================
        # STRATEGY 2: THE BLACKLIST AUDIT (Checking for Leakage)
        # ==============================================================================
        # We do NOT create features for these. We just check if they exist.
        # If "Desc_Cleared" is in the text, we ignore it. We don't let the model see it.
        
        leakage_keywords = {
            'LEAK_Cleared':   r'\b(cleared|reopen|gone|recover)\b',
            'LEAK_Towed':     r'\b(tow|wrecker|remove|hauled)\b',
            'LEAK_Hospital':  r'\b(hospital|injur|fatal|transport)\b' # Injuries often confirmed later
        }
        
        print("\n--- LEAKAGE AUDIT (For Validation Only) ---")
        for name, pattern in leakage_keywords.items():
            count = df_text['desc_clean'].str.contains(pattern, regex=True).sum()
            print(f"   [AUDIT] Rows containing '{name}' (Post-Crash info): {count}")
        
        print("\n   NOTE: The model will NOT see these leakage words. They remain hidden in the text.")
        
        # ==============================================================================
        # CLEANUP
        # ==============================================================================
        # Drop the raw text column (and the temporary clean column)
        # This guarantees the model CANNOT cheat. It only sees the 8 binary columns we created.
        if 'Description' in df_text.columns:
            df_text = df_text.drop(columns=['Description', 'desc_clean'])
        
        # Update the main dataframe
        df_final = df_text.copy()
        
        print(f"\nFinal Column Count: {len(df_final.columns)}")

        print(df_final.columns.to_list())

        df_final.head(2)

        
        # Create a figure with two side-by-side subplots
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. BEFORE: Plot Raw Precipitation
        # We filter for > 0 to see the shape of actual rain events (ignoring the massive spike at 0)
        raw_rain = df_final[df_final['Precipitation(in)'] > 0]['Precipitation(in)']
        sns.histplot(raw_rain, bins=50, kde=True, color='#E74C3C', ax=axes[0])
        axes[0].set_title('BEFORE: Precipitation Skewed', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Precipitation (inches)')
        axes[0].set_ylabel('Frequency')
        
        # 2. AFTER: Plot Log-Transformed Precipitation
        # We apply np.log1p() which calculates log(1 + x)
        log_rain = np.log1p(raw_rain)
        sns.histplot(log_rain, bins=50, kde=True, color='#3498DB', ax=axes[1])
        axes[1].set_title('AFTER: Log-Transformed (Normalized)', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Log(1 + Precipitation)')
        axes[1].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.show()

        
        
        # Create a checkpoint copy for safety
        df_audit_check = df_final.copy()
        
        print("--- PHASE 5a: THE 'SILENT KILLER' AUDIT ---")
        print(f"Initial Shape: {df_audit_check.shape}")
        
        # --- 1. INFINITY CHECK ---
        # Pandas and Sklearn choke on 'inf'. We must replace them with NaN.
        numeric_cols = df_audit_check.select_dtypes(include=np.number).columns
        n_inf = np.isinf(df_audit_check[numeric_cols]).sum().sum()
        
        print(f"\n1. Infinity Scan:")
        if n_inf > 0:
            print(f"   [CRITICAL] Found {n_inf} infinite values. FIXING...")
            # Replace inf with NaN
            df_audit_check = df_audit_check.replace([np.inf, -np.inf], np.nan)
            # Fill that NaN with the column median (safe imputation)
            df_audit_check = df_audit_check.fillna(df_audit_check.median(numeric_only=True))
            print("   -> [FIXED] Replaced infinite values with column medians.")
        else:
            print("   -> [PASS] No infinite values found.")
        
        # --- 2. BOOLEAN TYPE CHECK ---
        # ML models prefer integers (0/1) over boolean (True/False).
        bool_cols = df_audit_check.select_dtypes(include=['bool']).columns
        
        print(f"\n2. Boolean Type Scan:")
        if len(bool_cols) > 0:
            print(f"   [ACTION] Found {len(bool_cols)} boolean columns: {list(bool_cols)}")
            for col in bool_cols:
                df_audit_check[col] = df_audit_check[col].astype(int)
            print("   -> [FIXED] Converted all booleans to Integers (0/1).")
        else:
            print("   -> [PASS] No boolean columns found (already int?).")
        
        # --- 3. DATETIME TYPE CHECK ---
        # Ensure Start_Time is actual datetime for the next step (Cyclical Encoding)
        print(f"\n3. Datetime Check:")
        if df_audit_check['Start_Time'].dtype == 'O': # Object/String
            print("   [ACTION] 'Start_Time' is String. Converting to Datetime...")
            df_audit_check['Start_Time'] = pd.to_datetime(df_audit_check['Start_Time'])
            print("   -> [FIXED] Converted to Datetime.")
        else:
            print(f"   -> [PASS] 'Start_Time' is already {df_audit_check['Start_Time'].dtype}.")
        
        # --- 4. FINAL NULL CHECK ---
        # One last verify that no NaNs slipped through
        total_nans = df_audit_check.isnull().sum().sum()
        print(f"\n4. Final Null Check:")
        if total_nans > 0:
            print(f"   [CRITICAL] Found {total_nans} missing values remaining!")
            print(df_audit_check.isnull().sum()[df_audit_check.isnull().sum() > 0])
        else:
            print("   -> [PASS] Dataset is 100% CLEAN (0 Nulls).")
        
        # Update df_final with the audited version
        df_final = df_audit_check
        print("\n--- AUDIT COMPLETE. READY FOR TRANSFORMATION. ---")


        #full dataset transformation
        
        
        # Create df_trans as the numerical version of df_final
        df_trans = df_final.copy()
        
        print("--- BLOCK 1: GLOBAL NUMERICAL TRANSFORMATIONS ---")
        
        # 1. CYCLICAL TIME (The "Clock" Fix)
        # Hour (0-23) and Month (1-12) are circular. 23 is close to 0.
        if df_trans['Start_Time'].dtype == 'O':
            df_trans['Start_Time'] = pd.to_datetime(df_trans['Start_Time'])
        
        # Extract
        hour = df_trans['Start_Time'].dt.hour
        month = df_trans['Start_Time'].dt.month
        
        # Transform
        df_trans['Hour_Sin'] = np.sin(2 * np.pi * hour / 24)
        df_trans['Hour_Cos'] = np.cos(2 * np.pi * hour / 24)
        df_trans['Month_Sin'] = np.sin(2 * np.pi * month / 12)
        df_trans['Month_Cos'] = np.cos(2 * np.pi * month / 12)
        
        print("1. Time: Converted to Cyclical Sin/Cos.")
        
        # 2. LOG TRANSFORMATION (The "Skew" Fix)
        # Distance and Duration follow Power Laws. We log them to compress the outliers.
        for col in [ 'Precipitation(in)']:
            if col in df_trans.columns:
                # log1p handles 0s safely (log(0+1) = 0)
                df_trans[f'Log_{col}'] = np.log1p(df_trans[col])
                df_trans = df_trans.drop(columns=[col])
        print("2. Physics: Log-Transformed Precipitation.")
        
        # 3. BOOLEAN STANDARDIZATION
        # Convert all True/False to 1/0
        bool_cols = df_trans.select_dtypes(include=['bool']).columns
        for col in bool_cols:
            df_trans[col] = df_trans[col].astype(int)
        print(f"3. Booleans: Converted {len(bool_cols)} columns to Int.")
        
        # 4. ONE-HOT ENCODING (The "Structure" Fix)
        # We encode categorical text.
        # NOTE: We DROP High-Cardinality text (City, County, State) to prevent dimensionality explosion.
        cols_to_drop = ['City', 'County', 'State', 'Start_Time', 'End_Time', 
                        'Start_Lat', 'Start_Lng', 'Description', 'Street', 'Hour', 'Month'] # Dropping Lat/Lng to avoid "Map Bias"
        df_trans2 = df_trans.drop(columns=cols_to_drop, errors='ignore')
        
        # Encode remaining (Weather, Wind)
        df_trans2 = pd.get_dummies(df_trans2, columns=['Weather_Simplified', 'Wind_Direction'], drop_first=True, dtype=int)
        print("4. Encoding: One-Hot Encoded Weather/Wind. Dropped Geo/Text columns.")
        
        print("-" * 40)
        print(f"NUMERICAL MATRIX SHAPE: {df_trans2.shape}")

        print(df_trans2.columns.tolist())


        df_trans2.shape

        df_trans2.head()


        # Update state with current locals (naively saving everything)
        # Filtering to avoid saving self and state itself
        state.update({k: v for k, v in locals().items() if k not in ['self', 'state']})
        return state

--------------------
--- Cell 6 (Start_Lat Drop) ---
class FeatureEngineering:
    def __init__(self):
        pass

    def execute(self, state):
        print(f'--- Executing {self.__class__.__name__} ---')
        # Restore state variables to local scope
        globals().update(state)

        # --- FIX: Explicitly retrieve state variables to prevent UnboundLocalError ---
        if 'df_final' in state: df_final = state['df_final']
        # -----------------------------------------------------------------------------

        
        
        
        # Use a copy to avoid SettingWithCopy warnings
        df_text = df_final.copy()
        
        # Ensure text is lowercase for matching
        df_text['desc_clean'] = df_text['Description'].str.lower().fillna('')
        
        print("--- STARTING REAL-TIME TEXT ANALYSIS ---")
        
        # ==============================================================================
        # STRATEGY 1: THE WHITELIST (Safe Features known at T=0)
        # ==============================================================================
        # We only create columns for these specific, real-time physical states.
        
        safe_keywords = {
            # 1. TRAFFIC STATUS (The "Effect")
            'Desc_Queue':     r'\b(queue|backups?|slow|stationary|stop|waiting|delays?)\b',
            'Desc_Heavy':     r'\b(heavy|congestion|gridlock|bumper)\b',
            
            # 2. PHYSICAL BLOCKAGE (The "Cause")
            'Desc_Blocked':   r'\b(block|close|lane|closed|shut|down)\b',
            'Desc_Ramp':      r'\b(ramp|exit|entry|interchange)\b',
            
            # 3. INCIDENT TYPE (The "Event")
            'Desc_Accident':  r'\b(accident|crash|collision|incident)\b',
            'Desc_Hazard':    r'\b(hazard|debris|object|spill|obstacle|animal)\b',
            
            # 4. URGENCY / SPECIFICS
            'Desc_Caution':   r'\b(caution|care|alert|warning)\b',
            'Desc_Fire':      r'\b(fire|smoke|flame|burn)\b'
        }
        
        print("Extracting Real-Time Whitelist Features...")
        for col_name, pattern in safe_keywords.items():
            # Create 1/0 Feature
            df_text[col_name] = df_text['desc_clean'].str.contains(pattern, regex=True).astype(int)
            
            # Check "Variance": If a column is 99.9% 0s, it might be useless (optional check)
            count = df_text[col_name].sum()
            print(f"   -> Feature '{col_name}' detected in {count} rows.")
        
        # ==============================================================================
        # STRATEGY 2: THE BLACKLIST AUDIT (Checking for Leakage)
        # ==============================================================================
        # We do NOT create features for these. We just check if they exist.
        # If "Desc_Cleared" is in the text, we ignore it. We don't let the model see it.
        
        leakage_keywords = {
            'LEAK_Cleared':   r'\b(cleared|reopen|gone|recover)\b',
            'LEAK_Towed':     r'\b(tow|wrecker|remove|hauled)\b',
            'LEAK_Hospital':  r'\b(hospital|injur|fatal|transport)\b' # Injuries often confirmed later
        }
        
        print("\n--- LEAKAGE AUDIT (For Validation Only) ---")
        for name, pattern in leakage_keywords.items():
            count = df_text['desc_clean'].str.contains(pattern, regex=True).sum()
            print(f"   [AUDIT] Rows containing '{name}' (Post-Crash info): {count}")
        
        print("\n   NOTE: The model will NOT see these leakage words. They remain hidden in the text.")
        
        # ==============================================================================
        # CLEANUP
        # ==============================================================================
        # Drop the raw text column (and the temporary clean column)
        # This guarantees the model CANNOT cheat. It only sees the 8 binary columns we created.
        if 'Description' in df_text.columns:
            df_text = df_text.drop(columns=['Description', 'desc_clean'])
        
        # Update the main dataframe
        df_final = df_text.copy()
        
        print(f"\nFinal Column Count: {len(df_final.columns)}")

        print(df_final.columns.to_list())

        df_final.head(2)

        
        # Create a figure with two side-by-side subplots
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. BEFORE: Plot Raw Precipitation
        # We filter for > 0 to see the shape of actual rain events (ignoring the massive spike at 0)
        raw_rain = df_final[df_final['Precipitation(in)'] > 0]['Precipitation(in)']
        sns.histplot(raw_rain, bins=50, kde=True, color='#E74C3C', ax=axes[0])
        axes[0].set_title('BEFORE: Precipitation Skewed', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Precipitation (inches)')
        axes[0].set_ylabel('Frequency')
        
        # 2. AFTER: Plot Log-Transformed Precipitation
        # We apply np.log1p() which calculates log(1 + x)
        log_rain = np.log1p(raw_rain)
        sns.histplot(log_rain, bins=50, kde=True, color='#3498DB', ax=axes[1])
        axes[1].set_title('AFTER: Log-Transformed (Normalized)', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Log(1 + Precipitation)')
        axes[1].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.show()

        
        
        # Create a checkpoint copy for safety
        df_audit_check = df_final.copy()
        
        print("--- PHASE 5a: THE 'SILENT KILLER' AUDIT ---")
        print(f"Initial Shape: {df_audit_check.shape}")
        
        # --- 1. INFINITY CHECK ---
        # Pandas and Sklearn choke on 'inf'. We must replace them with NaN.
        numeric_cols = df_audit_check.select_dtypes(include=np.number).columns
        n_inf = np.isinf(df_audit_check[numeric_cols]).sum().sum()
        
        print(f"\n1. Infinity Scan:")
        if n_inf > 0:
            print(f"   [CRITICAL] Found {n_inf} infinite values. FIXING...")
            # Replace inf with NaN
            df_audit_check = df_audit_check.replace([np.inf, -np.inf], np.nan)
            # Fill that NaN with the column median (safe imputation)
            df_audit_check = df_audit_check.fillna(df_audit_check.median(numeric_only=True))
            print("   -> [FIXED] Replaced infinite values with column medians.")
        else:
            print("   -> [PASS] No infinite values found.")
        
        # --- 2. BOOLEAN TYPE CHECK ---
        # ML models prefer integers (0/1) over boolean (True/False).
        bool_cols = df_audit_check.select_dtypes(include=['bool']).columns
        
        print(f"\n2. Boolean Type Scan:")
        if len(bool_cols) > 0:
            print(f"   [ACTION] Found {len(bool_cols)} boolean columns: {list(bool_cols)}")
            for col in bool_cols:
                df_audit_check[col] = df_audit_check[col].astype(int)
            print("   -> [FIXED] Converted all booleans to Integers (0/1).")
        else:
            print("   -> [PASS] No boolean columns found (already int?).")
        
        # --- 3. DATETIME TYPE CHECK ---
        # Ensure Start_Time is actual datetime for the next step (Cyclical Encoding)
        print(f"\n3. Datetime Check:")
        if df_audit_check['Start_Time'].dtype == 'O': # Object/String
            print("   [ACTION] 'Start_Time' is String. Converting to Datetime...")
            df_audit_check['Start_Time'] = pd.to_datetime(df_audit_check['Start_Time'])
            print("   -> [FIXED] Converted to Datetime.")
        else:
            print(f"   -> [PASS] 'Start_Time' is already {df_audit_check['Start_Time'].dtype}.")
        
        # --- 4. FINAL NULL CHECK ---
        # One last verify that no NaNs slipped through
        total_nans = df_audit_check.isnull().sum().sum()
        print(f"\n4. Final Null Check:")
        if total_nans > 0:
            print(f"   [CRITICAL] Found {total_nans} missing values remaining!")
            print(df_audit_check.isnull().sum()[df_audit_check.isnull().sum() > 0])
        else:
            print("   -> [PASS] Dataset is 100% CLEAN (0 Nulls).")
        
        # Update df_final with the audited version
        df_final = df_audit_check
        print("\n--- AUDIT COMPLETE. READY FOR TRANSFORMATION. ---")


        #full dataset transformation
        
        
        # Create df_trans as the numerical version of df_final
        df_trans = df_final.copy()
        
        print("--- BLOCK 1: GLOBAL NUMERICAL TRANSFORMATIONS ---")
        
        # 1. CYCLICAL TIME (The "Clock" Fix)
        # Hour (0-23) and Month (1-12) are circular. 23 is close to 0.
        if df_trans['Start_Time'].dtype == 'O':
            df_trans['Start_Time'] = pd.to_datetime(df_trans['Start_Time'])
        
        # Extract
        hour = df_trans['Start_Time'].dt.hour
        month = df_trans['Start_Time'].dt.month
        
        # Transform
        df_trans['Hour_Sin'] = np.sin(2 * np.pi * hour / 24)
        df_trans['Hour_Cos'] = np.cos(2 * np.pi * hour / 24)
        df_trans['Month_Sin'] = np.sin(2 * np.pi * month / 12)
        df_trans['Month_Cos'] = np.cos(2 * np.pi * month / 12)
        
        print("1. Time: Converted to Cyclical Sin/Cos.")
        
        # 2. LOG TRANSFORMATION (The "Skew" Fix)
        # Distance and Duration follow Power Laws. We log them to compress the outliers.
        for col in [ 'Precipitation(in)']:
            if col in df_trans.columns:
                # log1p handles 0s safely (log(0+1) = 0)
                df_trans[f'Log_{col}'] = np.log1p(df_trans[col])
                df_trans = df_trans.drop(columns=[col])
        print("2. Physics: Log-Transformed Precipitation.")
        
        # 3. BOOLEAN STANDARDIZATION
        # Convert all True/False to 1/0
        bool_cols = df_trans.select_dtypes(include=['bool']).columns
        for col in bool_cols:
            df_trans[col] = df_trans[col].astype(int)
        print(f"3. Booleans: Converted {len(bool_cols)} columns to Int.")
        
        # 4. ONE-HOT ENCODING (The "Structure" Fix)
        # We encode categorical text.
        # NOTE: We DROP High-Cardinality text (City, County, State) to prevent dimensionality explosion.
        cols_to_drop = ['City', 'County', 'State', 'Start_Time', 'End_Time', 
                        'Start_Lat', 'Start_Lng', 'Description', 'Street', 'Hour', 'Month'] # Dropping Lat/Lng to avoid "Map Bias"
        df_trans2 = df_trans.drop(columns=cols_to_drop, errors='ignore')
        
        # Encode remaining (Weather, Wind)
        df_trans2 = pd.get_dummies(df_trans2, columns=['Weather_Simplified', 'Wind_Direction'], drop_first=True, dtype=int)
        print("4. Encoding: One-Hot Encoded Weather/Wind. Dropped Geo/Text columns.")
        
        print("-" * 40)
        print(f"NUMERICAL MATRIX SHAPE: {df_trans2.shape}")

        print(df_trans2.columns.tolist())


        df_trans2.shape

        df_trans2.head()


        # Update state with current locals (naively saving everything)
        # Filtering to avoid saving self and state itself
        state.update({k: v for k, v in locals().items() if k not in ['self', 'state']})
        return state

--------------------
